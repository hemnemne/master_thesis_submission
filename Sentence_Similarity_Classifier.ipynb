{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemnemne/master_thesis_submission/blob/main/Sentence_Similarity_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# are we doing services or faqs or topics?\n",
        "classifiers = [\"services\", \"faqs\", \"topics\", \"topics and services\"]\n",
        "# pick one\n",
        "classifier = classifiers[0]\n",
        "# how many user queries do we want to test the model on?\n",
        "how_many = 1000\n",
        "# are we removing the stopwords\n",
        "remove_stopwords = False\n",
        "# do we want to see the actual output?\n",
        "wanna_print = False\n",
        "# \"out of k\" results\n",
        "k = 25\n",
        "# see the GPU\n",
        "get_hardware_info = False\n",
        "# are we lower casing everything?\n",
        "do_lower_case = False\n",
        "# 0 is the default, so nothing is filtered. set to >1.0 to filter out any services that are not relevant accoarding to d115\n",
        "boost_value_threshold = .0\n",
        "# all available pooling modes\n",
        "pooling_modes = [\"cls\",\"mean\",\"max\",\"msl\"]\n",
        "# pick one\n",
        "pooling_mode = pooling_modes[1] \n",
        "# embedding dimension of the representation vectors\n",
        "dimension = 1024\n",
        "# random seed\n",
        "random_state=100\n",
        "# new k for the topic --> service classifier (for the SERVICES. topics is still the regular k above)\n",
        "k_topics = 25\n",
        "\n",
        "# available models\n",
        "models = ['Sahajtomar/German-semantic', # sota\n",
        "          'clips/mfaq', \n",
        "          'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli', \n",
        "          'deepset/gelectra-large-germanquad', \n",
        "          'deepset/gbert-large']\n",
        "current_model = models[0]\n",
        "\n",
        "parameters={\n",
        "    \"random_state\":random_state,\n",
        "    \"how_many\" : how_many,\n",
        "    \"wanna_print\":wanna_print, \n",
        "    \"get_hardware_info\" : get_hardware_info,\n",
        "    \"classifier\" : classifier,\n",
        "    \"remove_stopwords\":remove_stopwords, \n",
        "    \"k\":k, \n",
        "    \"do_lower_case\":do_lower_case,\n",
        "    \"current_model\":current_model,\n",
        "    \"boost_value_threshold\" : boost_value_threshold,\n",
        "    \"pooling_mode\":pooling_mode,\n",
        "    \"dimension\":dimension\n",
        "}"
      ],
      "metadata": {
        "id": "YiPIiw93knCN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if get_hardware_info:\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU')\n",
        "  else:\n",
        "    print(gpu_info)"
      ],
      "metadata": {
        "id": "ilOdPXL8CksQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7uNDd5EnHPm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ad448f-f0dd-4935-bf45-7db858a8c24f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# load the real dataset\n",
        "\n",
        "# sync google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/Masterarbeit/Colab_Data/LeiKa/\"\n",
        "\n",
        "# where is the file?\n",
        "file_path_services = base_path + \"services.csv\"\n",
        "file_path_faqs = base_path + \"faq.csv\"\n",
        "file_path_topics = base_path + \"topics.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "K3iGeM9ibxp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c1fb05-fb1c-4a4c-94a3-5ad850dcd5cb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.23.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VaU2mYvPIKxM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# get all services\n",
        "\n",
        "df_services = pd.read_csv(file_path_services)\n",
        "\n",
        "if boost_value_threshold > 1.0:\n",
        "  # and now filter the ones that have a boost value\n",
        "  df_services = df_services[df_services[[i for i in df_services.columns if i[:4] == \"d115\"]][\"d115DocumentBoostValue\"]>boost_value_threshold].sample(frac=1.,random_state=random_state)\n",
        "\n",
        "# get the faqs\n",
        "\n",
        "df_faqs = pd.read_csv(file_path_faqs)\n",
        "\n",
        "# get the topics\n",
        "\n",
        "df_topics = pd.read_csv(file_path_topics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here come the stopwords\n",
        "\n",
        "if remove_stopwords:\n",
        "  import nltk\n",
        "  from nltk.corpus import stopwords\n",
        "  nltk.download('stopwords')\n",
        "  german_stop_words = stopwords.words('german')"
      ],
      "metadata": {
        "id": "ZEQGPXrcj26Q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stop_word_removal(x):\n",
        "  token = x.split()\n",
        "  return ' '.join([w for w in token if not w in german_stop_words])"
      ],
      "metadata": {
        "id": "QkZUR_ZZkOGQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "def pytorch_cos_sim(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    return cos_sim(a, b)\n",
        "\n",
        "def cos_sim(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "\n",
        "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
        "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
        "    return torch.mm(a_norm, b_norm.transpose(0, 1))"
      ],
      "metadata": {
        "id": "C_phhkERejYD"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer(current_model)"
      ],
      "metadata": {
        "id": "3unW4ThTb8vx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is all for the topics. It maps them to the corresponding services\n",
        "\n",
        "def get_fibel():\n",
        "  topics_fibel = {} \n",
        "  for row in df_topics.iterrows():\n",
        "    row = row[1]\n",
        "    category = row[\"d115Name\"]\n",
        "    services = row[\"d115Services\"]\n",
        "    if type(services) == float:\n",
        "      continue\n",
        "    service_ids = []\n",
        "    for service in services.split('\"'):\n",
        "      try: \n",
        "        service_ids.append(int(service))\n",
        "      except ValueError:\n",
        "        continue\n",
        "    topics_fibel[category] = service_ids\n",
        "  return topics_fibel\n",
        "\n",
        "def get_topics_with_services_as_df() -> pd.DataFrame:\n",
        "  \n",
        "  topics_fibel = get_fibel()\n",
        "  topics_fibel_transposed = []\n",
        "  for key, value in topics_fibel.items():\n",
        "    for i in value:\n",
        "      topics_fibel_transposed.append([i,key])\n",
        "  return pd.DataFrame(topics_fibel_transposed,columns=[\"service_id\", \"topic\"])"
      ],
      "metadata": {
        "id": "EHu-MLGZYG71"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wmLtxACpZuDn"
      },
      "outputs": [],
      "source": [
        "# we get the log file\n",
        "\n",
        "path = \"/content/drive/MyDrive/Masterarbeit/Colab_Data/log.csv\"\n",
        "test_data = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get rid of the \"-\" and \"NaN\" values in the ID column\n",
        "\n",
        "new_id = []\n",
        "for i in test_data[\"selectedID\"].to_list():\n",
        "  try:\n",
        "    new_id.append(int(i))\n",
        "  except ValueError:\n",
        "    new_id.append(0)\n",
        "test_data[\"selectedID\"] = new_id"
      ],
      "metadata": {
        "id": "ayWwCQR4lVbM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we merge the log (test_data) with the actual services and topics to not only have the selected service -ID-, but also its corresponding -name- and -topic-\n",
        "\n",
        "if classifier != \"faqs\":\n",
        "  test_data = test_data[(test_data[\"userQuestion\"]==\"SERVICE_SELECTION_REQUEST\") & (test_data[\"selectedID\"]>0)]\n",
        "  test_data = test_data.merge(df_services, left_on=\"selectedID\", right_on=\"id\")\n",
        "  test_data = test_data.merge(get_topics_with_services_as_df(), left_on=\"selectedID\", right_on=\"service_id\")"
      ],
      "metadata": {
        "id": "EKdyetiKynuV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store the availiable labels in the \"labels\" list\n",
        "\n",
        "if classifier == \"services\":\n",
        "  labels = df_services[\"d115Name\"].to_list()\n",
        "  distribution = test_data[\"d115Name\"].value_counts()\n",
        "elif classifier == \"faqs\":\n",
        "  test_data = test_data[test_data[\"userQuestion\"] == \"FAQ_ANSWER\"]\n",
        "  labels = df_faqs[\"faqQuestion\"].to_list()\n",
        "elif classifier == \"topics\" or classifier == \"topics and services\":\n",
        "  distribution = test_data[\"topic\"].value_counts()\n",
        "  labels = list(get_fibel().keys())\n",
        "\n",
        "# shuffle the test data\n",
        "test_data = test_data.sample(frac=1.0,random_state=random_state)"
      ],
      "metadata": {
        "id": "AiCi05BaotHL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model[0].do_lower_case = do_lower_case\n",
        "\n",
        "model[1].word_embedding_dimension = dimension\n",
        "\n",
        "# now, we set the pooling mode. this looks complicated but simply sets the selected one to True and all the rest to False\n",
        "\n",
        "if pooling_mode == \"cls\":\n",
        "  model[1].pooling_mode_cls_token = True\n",
        "  model[1].pooling_mode_mean_tokens = model[1].pooling_mode_max_tokens = model[1].pooling_mode_mean_sqrt_len_tokens = False\n",
        "elif pooling_mode == \"mean\":\n",
        "  model[1].pooling_mode_mean_tokens = True\n",
        "  model[1].pooling_mode_cls_token = model[1].pooling_mode_max_tokens = model[1].pooling_mode_mean_sqrt_len_tokens = False\n",
        "elif pooling_mode == \"max\":\n",
        "  model[1].pooling_mode_max_tokens = True\n",
        "  model[1].pooling_mode_cls_token = model[1].pooling_mode_mean_tokens = model[1].pooling_mode_mean_sqrt_len_tokens = False\n",
        "elif pooling_mode == \"msl\":\n",
        "  model[1].pooling_mode_mean_sqrt_len_tokens = True\n",
        "  model[1].pooling_mode_cls_token = model[1].pooling_mode_mean_tokens = model[1].pooling_mode_max_tokens = False\n",
        "else:\n",
        "  raise ValueError(f\"The pooling mode {pooling_mode} is not deefined correctly.\")\n",
        "\n",
        "print(model[0])\n",
        "print(model[1])"
      ],
      "metadata": {
        "id": "MyRB49Z3k6ZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95e0655-46ed-4d9c-f8f0-b736553bc0c3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
            "Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"There are {len(labels)} possible labels in total.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jChyfUFR5PLz",
        "outputId": "d09594dc-89f4-4593-cab6-65d3ea63893f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 881 possible labels in total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_embedded = model.encode(labels, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "lNrPK_VsgQ3l"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if we are looking for the topic fist and then the service\n",
        "\n",
        "# if classifier == \"topics and services\":\n",
        "topic_labels_embedded = {}\n",
        "fibel = get_fibel()\n",
        "\n",
        "# function to get the service name from an id\n",
        "def get_service_name_from_id(id:int):\n",
        "  for i in df_services.iterrows():\n",
        "    if i[1][\"id\"] == id:\n",
        "      return i[1][\"d115Name\"]\n",
        "\n",
        "# for a topic (as str) this gets the list of str of all services\n",
        "def get_service_names_from_topic_name(topic:str):\n",
        "  return [get_service_name_from_id(i) for i in fibel[topic]]\n",
        "\n",
        "for topic in list(fibel.keys()):\n",
        "  topic_labels_embedded[topic] = model.encode(get_service_names_from_topic_name(topic), convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "m86z_e9B7WAS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "vPFrSw4ipZ9S"
      },
      "outputs": [],
      "source": [
        "hits = 0\n",
        "\n",
        "invest_len_df = pd.DataFrame(columns=[\"query\", \"length\", \"correct\"])\n",
        "\n",
        "for index,row in test_data.iloc[:how_many].iterrows():\n",
        "  question = row[\"sessionID\"]\n",
        "  if remove_stopwords:\n",
        "    question = stop_word_removal(question)\n",
        "  query_embedded = model.encode(question, convert_to_tensor=True)\n",
        "  if classifier == \"services\" or classifier == \"topics and services\":\n",
        "    true_answer = row[\"d115Name\"]\n",
        "  elif classifier == \"topics\":\n",
        "    true_answer = row[\"topic\"]\n",
        "  elif classifier == \"faqs\":\n",
        "    # for the FAQs we use a different csv we pull the data from (find the code next block)\n",
        "    break\n",
        "  cosine_scores = util.pytorch_cos_sim(query_embedded, labels_embedded)\n",
        "  scores = cosine_scores[0].tolist()\n",
        "  score_df = pd.DataFrame(columns=[\"service\", \"score\"])\n",
        "  score_df[\"service\"] = labels\n",
        "  score_df[\"score\"] = scores\n",
        "  ranking = score_df.sort_values(by=\"score\",ascending=False)\n",
        "  mvps = ranking.iloc[:k]\n",
        "\n",
        "  if classifier == \"topics and services\":\n",
        "\n",
        "    # this is if we not only classify topics, but also their services\n",
        "    \n",
        "    servs = pd.DataFrame(columns=[\"service\",\"score\"])\n",
        "    for i in mvps[\"service\"].to_list():\n",
        "      cosine_scores = util.pytorch_cos_sim(query_embedded, topic_labels_embedded[i])\n",
        "      scores = cosine_scores[0].tolist()\n",
        "      score_df = pd.DataFrame(columns=[\"service\", \"score\"])\n",
        "      score_df[\"service\"] = get_service_names_from_topic_name(i)\n",
        "      score_df[\"score\"] = scores\n",
        "      ranking = score_df.sort_values(by=\"score\",ascending=False)\n",
        "      mvps_for_each_topic = ranking.iloc[:k]\n",
        "      servs = servs.append(mvps_for_each_topic)\n",
        "    servs = servs.sort_values(by=\"score\",ascending=False)\n",
        "\n",
        "    mvps = servs.iloc[:k_topics]\n",
        "\n",
        "  invest_len_dict = {}\n",
        "  invest_len_dict[\"query\"] = question\n",
        "  invest_len_dict[\"length\"] = len(question)\n",
        "  if true_answer in mvps['service'].to_list() and classifier != \"faqs\":\n",
        "    # increase the hits\n",
        "    hits += 1\n",
        "    # and catch the query to investigate the length correlation\n",
        "    invest_len_dict[\"correct\"] = 1\n",
        "  else:\n",
        "    invest_len_dict[\"correct\"] = 0\n",
        "  invest_len_df = invest_len_df.append(pd.Series(invest_len_dict),ignore_index=True)\n",
        "  if wanna_print:\n",
        "    print(question)\n",
        "    print(true_answer)\n",
        "    print(mvps)\n",
        "    print(\"\\n\\n\\n\")\n",
        "\n",
        "if classifier != \"faqs\":\n",
        "  score = (hits/how_many)*100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA6dr1Sfdk1S",
        "outputId": "da9b33e9-e7bf-46db-ace1-42f235d15860"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the new data\n",
        "\n",
        "if classifier == \"faqs\":\n",
        "\n",
        "  faq_path = \"/content/drive/MyDrive/Masterarbeit/Colab_Data/faqClicks.csv\"\n",
        "  new_faq_test_data = pd.read_csv(faq_path).drop_duplicates(subset=[\"ursprüngliche Frage\"]).sample(frac=1.,random_state=random_state)\n",
        "  queries = new_faq_test_data[\"ursprüngliche Frage\"].to_list()\n",
        "  true_answers = new_faq_test_data[\"angeklickte FAQ-Frage\"].to_list()\n",
        "  possible_answers = [i for i in new_faq_test_data[\"angeklickte FAQ-Frage\"].unique()]\n",
        "\n",
        "  answers_embedded = model.encode(possible_answers, convert_to_tensor=True)\n",
        "  hits = 0\n",
        "\n",
        "  for idx,value in enumerate(queries[:how_many]):\n",
        "    if remove_stopwords:\n",
        "      question = stop_word_removal(value)\n",
        "    query_embedded = model.encode(question, convert_to_tensor=True)\n",
        "    cosine_scores = util.pytorch_cos_sim(query_embedded, answers_embedded)\n",
        "    scores = cosine_scores[0].tolist()\n",
        "\n",
        "    score_df = pd.DataFrame(columns=[\"service\", \"score\"])\n",
        "    score_df[\"service\"] = possible_answers\n",
        "    score_df[\"score\"] = scores\n",
        "    ranking = score_df.sort_values(by=\"score\",ascending=False)\n",
        "    mvps = ranking.iloc[:k]\n",
        "    if true_answers[idx] in mvps[\"service\"].to_list():\n",
        "      if wanna_print:\n",
        "        print(\"Correct:\\n\")\n",
        "      hits += 1\n",
        "    else:\n",
        "      if wanna_print:\n",
        "        print(\"Wrong:\\n\")\n",
        "      pass\n",
        "    if wanna_print:\n",
        "      print(question)\n",
        "      print(true_answers[idx])\n",
        "      print(mvps)\n",
        "      print(\"\\n\\n\")\n",
        "  score = (hits/how_many)*100"
      ],
      "metadata": {
        "id": "KUZ35Ql6ywHH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add the score\n",
        "parameters[\"score\"] = score\n",
        "# cast to series\n",
        "results_row = pd.Series(parameters)"
      ],
      "metadata": {
        "id": "5NFLjplIfsmk"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_row"
      ],
      "metadata": {
        "id": "nFZe5ezCZGcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fa5200-041f-4a83-9dff-be3380c86632"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "random_state                                    100\n",
              "how_many                                       1000\n",
              "wanna_print                                   False\n",
              "get_hardware_info                             False\n",
              "classifier                                 services\n",
              "remove_stopwords                              False\n",
              "k                                                25\n",
              "do_lower_case                                 False\n",
              "current_model            Sahajtomar/German-semantic\n",
              "boost_value_threshold                           0.0\n",
              "pooling_mode                                   mean\n",
              "dimension                                      1024\n",
              "score                                          85.6\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save to frame\n",
        "results_path = \"/content/drive/MyDrive/Masterarbeit/Colab_Data/results_SCC.csv\"\n",
        "frame = pd.read_csv(results_path,index_col=False)\n",
        "frame = frame.append(results_row,ignore_index=True)\n",
        "frame.to_csv(results_path, index=False)"
      ],
      "metadata": {
        "id": "cqmT63ZQXWli"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the service distribution for Section 5.4.4\n",
        "investigate = False\n",
        "\n",
        "if investigate:\n",
        "  threshold = 10\n",
        "  distr = test_data[\"d115Name\"].value_counts()\n",
        "  n = sum(distr.to_list())\n",
        "  distr_dict = dict(distr)\n",
        "  counter_irrelevant = 0\n",
        "  for key,value in distr_dict.items():\n",
        "    if value < threshold:\n",
        "      counter_irrelevant += 1\n",
        "    print(f\"{key}:{round((value/n)*100,2)}\")\n",
        "  print(\"\\n\\n\")\n",
        "  print(f\"{counter_irrelevant} services out of {len(distr.index.to_list())} selected services have been selected less than {threshold} times\\n\\n\")\n",
        "  distr.plot(kind=\"pie\")"
      ],
      "metadata": {
        "id": "VT9UkXz5gtr7"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# investigate the correlation between the sequence length and correct classifications\n",
        "\n",
        "invest_len = False\n",
        "\n",
        "if invest_len:\n",
        "\n",
        "  print(invest_len_df)\n",
        "  correlation = invest_len_df[\"correct\"].astype(float).corr(invest_len_df[\"length\"].astype(float))\n",
        "\n",
        "  print(f\"The correlation between the correctness of the SCC and the length of the user query is {round(correlation,4)}\")"
      ],
      "metadata": {
        "id": "UiP0eAe6lGzx"
      },
      "execution_count": 52,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1qCcXXiM1FvP6VSw9OX25",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}