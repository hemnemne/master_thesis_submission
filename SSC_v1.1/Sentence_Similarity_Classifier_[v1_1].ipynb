{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemnemne/master_thesis_submission/blob/main/SSC_v1.1/Sentence_Similarity_Classifier_%5Bv1_1%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "YTLykc_7k8iY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "_BiSjpjxnw9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_parameters():\n",
        "  \"\"\"\n",
        "  We define all parameters and set them. \n",
        "  Afterwards we return a dictionary containing the current parameters\n",
        "  \"\"\"\n",
        "\n",
        "  # are we doing services or faqs or topics?\n",
        "  classifiers = [\"services\", \"faqs\", \"topics\", \"topics and services\"]\n",
        "  # pick one\n",
        "  classifier = classifiers[0]\n",
        "  # how many user queries do we want to test the model on?\n",
        "  how_many = 1000\n",
        "  # are we removing the stopwords\n",
        "  remove_stopwords = False\n",
        "  # do we want to see the actual output?\n",
        "  wanna_print = True\n",
        "  # \"out of k\" results\n",
        "  k = 25\n",
        "  # see the GPU\n",
        "  get_hardware_info = False\n",
        "  # are we lower casing everything?\n",
        "  do_lower_case = False\n",
        "  # 0 is the default, so nothing is filtered. set to >1.0 to filter out any services that are not relevant accoarding to d115\n",
        "  boost_value_threshold = .0\n",
        "  # all available pooling modes\n",
        "  pooling_modes = [\"cls\",\"mean\",\"max\",\"msl\"]\n",
        "  # pick one\n",
        "  pooling_mode = pooling_modes[1] \n",
        "  # embedding dimension of the representation vectors\n",
        "  dimension = 1024\n",
        "  # random seed\n",
        "  random_state=100\n",
        "  # new k for the topic --> service classifier (for the SERVICES. topics is still the regular k above)\n",
        "  k_topics = 10\n",
        "\n",
        "  # available models\n",
        "  models = ['Sahajtomar/German-semantic', # sota\n",
        "            'clips/mfaq', \n",
        "            'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli', \n",
        "            'deepset/gelectra-large-germanquad', \n",
        "            'deepset/gbert-large']\n",
        "  current_model = models[0]\n",
        "\n",
        "  parameters={\n",
        "      \"random_state\":random_state,\n",
        "      \"how_many\" : how_many,\n",
        "      \"wanna_print\":wanna_print, \n",
        "      \"get_hardware_info\" : get_hardware_info,\n",
        "      \"classifier\" : classifier,\n",
        "      \"remove_stopwords\":remove_stopwords, \n",
        "      \"k\":k, \n",
        "      \"k_topics\" : k_topics,\n",
        "      \"do_lower_case\":do_lower_case,\n",
        "      \"current_model\":current_model,\n",
        "      \"boost_value_threshold\" : boost_value_threshold,\n",
        "      \"pooling_mode\":pooling_mode,\n",
        "      \"dimension\":dimension\n",
        "  }\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "YiPIiw93knCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "erRSPqU7nsJJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uNDd5EnHPm4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_data(base_path = \n",
        "              \"/content/drive/MyDrive/Masterarbeit/Colab_Data/LeiKa/\"):\n",
        "  \"\"\"\n",
        "  Get all relevant files. \n",
        "  You need to connect the Google Drive containing the files here.\n",
        "  Returns the services, topics and FAQs each as a DataFrame. \n",
        "  \"\"\"\n",
        "\n",
        "  # load the real dataset\n",
        "\n",
        "  # sync google Drive\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")\n",
        "\n",
        "  base_path = base_path\n",
        "\n",
        "  # where is the file?\n",
        "  file_path_services = base_path + \"services.csv\"\n",
        "  file_path_faqs = base_path + \"faq.csv\"\n",
        "  file_path_topics = base_path + \"topics.csv\"\n",
        "\n",
        "  # get all services\n",
        "\n",
        "  df_services = pd.read_csv(file_path_services)\n",
        "\n",
        "  if parameters[\"boost_value_threshold\"] > 1.0:\n",
        "    # and now filter the ones that have a boost value\n",
        "    df_services = df_services[df_services[[i for i in df_services.columns if i[:4] == \"d115\"]][\"d115DocumentBoostValue\"]>boost_value_threshold].sample(frac=1.,random_state=random_state)\n",
        "\n",
        "  # get the faqs\n",
        "\n",
        "  df_faqs = pd.read_csv(file_path_faqs)\n",
        "\n",
        "  # get the topics\n",
        "\n",
        "  df_topics = pd.read_csv(file_path_topics)\n",
        "\n",
        "  return df_services, df_faqs, df_topics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is all for the topics. It maps them to the corresponding services\n",
        "\n",
        "def get_fibel(df_topics):\n",
        "  \"\"\"\n",
        "  Here, we generate s dictionary that has \n",
        "  the topics as keys and the service IDs as values. \n",
        "  \"\"\"\n",
        "\n",
        "  topics_fibel = {} \n",
        "  for row in df_topics.iterrows():\n",
        "    row = row[1]\n",
        "    category = row[\"d115Name\"]\n",
        "    services = row[\"d115Services\"]\n",
        "    if type(services) == float:\n",
        "      continue\n",
        "    service_ids = []\n",
        "    for service in services.split('\"'):\n",
        "      try: \n",
        "        service_ids.append(int(service))\n",
        "      except ValueError:\n",
        "        continue\n",
        "    topics_fibel[category] = service_ids\n",
        "\n",
        "  return topics_fibel\n",
        "\n",
        "def get_topics_with_services_as_df(df_topics) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Cast the dictionary into a DataFrame which makes it easier to use it \n",
        "  for later joins. \n",
        "  \"\"\"\n",
        "  \n",
        "  topics_fibel = get_fibel(df_topics)\n",
        "  topics_fibel_transposed = []\n",
        "  for key, value in topics_fibel.items():\n",
        "    for i in value:\n",
        "      topics_fibel_transposed.append([i,key])\n",
        "\n",
        "  return pd.DataFrame(topics_fibel_transposed,columns=[\"service_id\", \"topic\"])"
      ],
      "metadata": {
        "id": "sbER9t5so8R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_faqs(parameters,\n",
        "                 faq_path = \n",
        "                 \"/content/drive/MyDrive/Masterarbeit/Colab_Data/faqClicks.csv\"):\n",
        "  \"\"\"\n",
        "  Here, we get the NEW FAQ data that actually has clicks in it. \n",
        "  Now, we know what the user clicked and thus, what the 'real' answer is.\n",
        "  \"\"\"\n",
        "\n",
        "  new_faq_test_data = pd.read_csv(faq_path).drop_duplicates(subset=[\"ursprüngliche Frage\"])\n",
        "  new_faq_test_data.sample(frac=1., random_state=parameters[\"random_state\"])\n",
        "  \n",
        "  return new_faq_test_data\n",
        "\n",
        "def get_test_data(parameters, df_services, df_faqs, df_topics,\n",
        "                  path=\n",
        "                  \"/content/drive/MyDrive/Masterarbeit/Colab_Data/log.csv\",\n",
        "                  ):\n",
        "  \"\"\"\n",
        "  Get the log data to test the SCC later. \n",
        "  Also, get all possible labels from the log.\n",
        "  If the classifier is \"topics and services\", we also create a dictionary\n",
        "  with all topics as keys and the embedded services as values. \n",
        "  \"\"\"\n",
        "\n",
        "  # we get the log file\n",
        "\n",
        "  path = path\n",
        "  test_data = pd.read_csv(path)\n",
        "\n",
        "  # get rid of the \"-\" and \"NaN\" values in the ID column\n",
        "\n",
        "  new_id = []\n",
        "  for i in test_data[\"selectedID\"].to_list():\n",
        "    try:\n",
        "      new_id.append(int(i))\n",
        "    except ValueError:\n",
        "      new_id.append(0)\n",
        "  test_data[\"selectedID\"] = new_id\n",
        "\n",
        "  # we merge the log (test_data) with the actual services and topics to not only have the selected service -ID-, but also its corresponding -name- and -topic-\n",
        "\n",
        "  if parameters[\"classifier\"] != \"faqs\":\n",
        "    test_data = test_data[(test_data[\"userQuestion\"]==\"SERVICE_SELECTION_REQUEST\") & (test_data[\"selectedID\"]>0)]\n",
        "    test_data = test_data.merge(df_services, left_on=\"selectedID\", right_on=\"id\")\n",
        "    test_data = test_data.merge(get_topics_with_services_as_df(df_topics), left_on=\"selectedID\", right_on=\"service_id\")\n",
        "    \n",
        "    # now, we need the question and true answer all be in the same column, \n",
        "    # independent on the classifier \n",
        "    test_data[\"questions\"] = test_data[\"sessionID\"]\n",
        "\n",
        "    if parameters[\"classifier\"] == \"topics\":\n",
        "      test_data[\"true_answers\"] = test_data[\"topic\"]\n",
        "    else:\n",
        "      test_data[\"true_answers\"] = test_data[\"d115Name\"]\n",
        "\n",
        "  else:\n",
        "    test_data = get_new_faqs(parameters)\n",
        "\n",
        "    test_data[\"questions\"] = test_data[\"ursprüngliche Frage\"]\n",
        "    test_data[\"true_answers\"] = test_data[\"angeklickte FAQ-Frage\"]\n",
        "\n",
        "  # shuffle the test data\n",
        "  test_data = test_data.sample(frac=1.0,random_state=parameters[\"random_state\"])\n",
        "  \n",
        "  return test_data"
      ],
      "metadata": {
        "id": "aBiK9m16pbFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for a topic (as str) this gets the list of str of all services\n",
        "def public_get_service_names_from_topic_name(topic:str, df_services, df_topics, fibel):\n",
        "\n",
        "  def get_service_name_from_id_here(id:int):\n",
        "    for i in df_services.iterrows():\n",
        "      if i[1][\"id\"] == id:\n",
        "        return i[1][\"d115Name\"]\n",
        "        \n",
        "  return [get_service_name_from_id_here(i) for i in fibel[topic]]\n",
        "\n",
        "# embedd the labels\n",
        "\n",
        "def embed_labels(labels, parameters, df_services, df_topics):\n",
        "  if parameters[\"classifier\"] == \"topics and services\":\n",
        "\n",
        "    # if we are looking for the topic fist and then the service\n",
        "\n",
        "    topic_labels_embedded = {}\n",
        "    fibel = get_fibel(df_topics)\n",
        "\n",
        "    # function to get the service name from an id\n",
        "    def get_service_name_from_id(id:int):\n",
        "      for i in df_services.iterrows():\n",
        "        if i[1][\"id\"] == id:\n",
        "          return i[1][\"d115Name\"]\n",
        "\n",
        "    # for a topic (as str) this gets the list of str of all services\n",
        "    def get_service_names_from_topic_name(topic:str):\n",
        "      return [get_service_name_from_id(i) for i in fibel[topic]]\n",
        "\n",
        "    for topic in list(fibel.keys()):\n",
        "      topic_labels_embedded[topic] = model.encode(get_service_names_from_topic_name(topic), convert_to_tensor=True)\n",
        "      labels_embedded = topic_labels_embedded\n",
        "  else:\n",
        "    labels_embedded = model.encode(labels, convert_to_tensor=True)\n",
        "\n",
        "  return labels_embedded"
      ],
      "metadata": {
        "id": "w0HPjc-hRadk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "_sJnXjamn0iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "\n",
        "%pip install -U sentence-transformers\n",
        "%pip install nltk"
      ],
      "metadata": {
        "id": "K3iGeM9ibxp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230b2ba6-aac9-435f-ab3c-3408c4c844c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.23.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifier"
      ],
      "metadata": {
        "id": "OvM7eQcon4M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here come the stopwords\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "german_stop_words = stopwords.words('german')"
      ],
      "metadata": {
        "id": "ZEQGPXrcj26Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b80657-0d6d-4fa3-a431-87851d659e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stop_word_removal(x):\n",
        "  \"\"\"\n",
        "  Here, we remove all stop words from a String. \n",
        "  \"\"\"\n",
        "\n",
        "  token = x.split()\n",
        "  \n",
        "  return ' '.join([w for w in token if not w in german_stop_words])"
      ],
      "metadata": {
        "id": "QkZUR_ZZkOGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "def pytorch_cos_sim(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    return cos_sim(a, b)\n",
        "\n",
        "def cos_sim(a: Tensor, b: Tensor):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
        "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
        "    \"\"\"\n",
        "    if not isinstance(a, torch.Tensor):\n",
        "        a = torch.tensor(a)\n",
        "\n",
        "    if not isinstance(b, torch.Tensor):\n",
        "        b = torch.tensor(b)\n",
        "\n",
        "    if len(a.shape) == 1:\n",
        "        a = a.unsqueeze(0)\n",
        "\n",
        "    if len(b.shape) == 1:\n",
        "        b = b.unsqueeze(0)\n",
        "\n",
        "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
        "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
        "    return torch.mm(a_norm, b_norm.transpose(0, 1))"
      ],
      "metadata": {
        "id": "C_phhkERejYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def get_model(parameters:dict):\n",
        "\n",
        "  model = SentenceTransformer(parameters[\"current_model\"])\n",
        "\n",
        "  model[0].do_lower_case = parameters[\"do_lower_case\"]\n",
        "\n",
        "  model[1].word_embedding_dimension = parameters[\"dimension\"]\n",
        "\n",
        "  # now, we set the pooling mode. this looks complicated but simply sets the selected one to True and all the rest to False\n",
        "\n",
        "  if parameters[\"pooling_mode\"] == \"cls\":\n",
        "    model[1].pooling_mode_cls_token = True\n",
        "    model[1].pooling_mode_mean_tokens = model[1].pooling_mode_max_tokens = model[1].pooling_mode_mean_sqrt_len_tokens = False\n",
        "  elif parameters[\"pooling_mode\"] == \"mean\":\n",
        "    model[1].pooling_mode_mean_tokens = True\n",
        "    model[1].pooling_mode_cls_token = model[1].pooling_mode_max_tokens = model[1].pooling_mode_mean_sqrt_len_tokens = False\n",
        "  elif parameters[\"pooling_mode\"] == \"max\":\n",
        "    model[1].pooling_mode_max_tokens = True\n",
        "    model[1].pooling_mode_cls_token = model[1].pooling_mode_mean_tokens = model[1].pooling_mode_mean_sqrt_len_tokens = False\n",
        "  elif parameters[\"pooling_mode\"] == \"msl\":\n",
        "    model[1].pooling_mode_mean_sqrt_len_tokens = True\n",
        "    model[1].pooling_mode_cls_token = model[1].pooling_mode_mean_tokens = model[1].pooling_mode_max_tokens = False\n",
        "  else:\n",
        "    raise ValueError(f\"The pooling mode is not deefined correctly.\")\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "3unW4ThTb8vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(parameters, df_services, df_faqs, df_topics):\n",
        "  \"\"\"\n",
        "  Gets all labels as text from the dataframes, depending on the parameters. \n",
        "  Returns a list of labels (which can then be embedded).\n",
        "  \"\"\"\n",
        "\n",
        "  # store the availiable labels in the \"labels\" list\n",
        "\n",
        "  if parameters[\"classifier\"] == \"services\":\n",
        "    labels = df_services[\"d115Name\"].to_list()\n",
        "  elif parameters[\"classifier\"] == \"topics\" or parameters[\"classifier\"] == \"topics and services\":\n",
        "    # for the 'topics and services', the first classification will be topics\n",
        "    # i.e., they are treated the same for now\n",
        "    labels = list(get_fibel(df_topics).keys())\n",
        "  else:\n",
        "    faqs = get_new_faqs(parameters)\n",
        "    labels = [i for i in faqs[\"angeklickte FAQ-Frage\"].unique()]\n",
        "\n",
        "  return labels"
      ],
      "metadata": {
        "id": "LTASPActNPzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_fibel(df_topics):\n",
        "\n",
        "  new_fibel = {}\n",
        "\n",
        "  for row in df_topics.iterrows():\n",
        "    services = row[1][\"d115Services\"]\n",
        "    if type(services) == str:\n",
        "      topic = row[1][\"d115Name\"]\n",
        "      new_fibel[topic] = {}\n",
        "      service_ids = []\n",
        "      small_list = services.split('\"')\n",
        "      for idx, val in enumerate(small_list):\n",
        "        try: \n",
        "          service_number = int(val)\n",
        "          service_ids.append(service_number)\n",
        "          service_name = small_list[idx+4]\n",
        "          new_fibel[topic][service_number] = service_name\n",
        "        except ValueError:\n",
        "          continue\n",
        "  return new_fibel"
      ],
      "metadata": {
        "id": "4rNNKiMzBKbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify"
      ],
      "metadata": {
        "id": "a-Z-N6__uRAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the current parameter settings\n",
        "parameters = get_parameters()\n",
        "\n",
        "# get the dataframes\n",
        "df_services, df_faqs, df_topics = get_data()\n",
        "\n",
        "# get the log\n",
        "test_data = get_test_data(parameters, df_services, df_faqs, df_topics)\n",
        "\n",
        "# load the model\n",
        "model = get_model(parameters=parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EimUl82Tlfzj",
        "outputId": "70fa94f6-4bdd-470d-ba61-0b63f79b9ed6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for classifier in [\"topics and services\"]: #  [\"services\", \"faqs\", \"topics\", \"topics and services\"]: todo change back\n",
        "\n",
        "  if classifier == \"faqs\":\n",
        "    parameters[\"k\"] = 50\n",
        "    parameters[\"remove_stopwords\"] = True\n",
        "    parameters[\"pooling_mode\"] = \"mean\"\n",
        "  else:\n",
        "    parameters[\"k\"] = 25\n",
        "    parameters[\"remove_stopwords\"] = False\n",
        "    if classifier == \"topics and services\":\n",
        "      parameters[\"pooling_mode\"] = \"cls\"\n",
        "    else:\n",
        "      parameters[\"pooling_mode\"] = \"msl\"\n",
        "\n",
        "\n",
        "  sample_size = 1000 # parameters[\"how_many\"] todo change back\n",
        "\n",
        "  hits = 0\n",
        "\n",
        "  parameters[\"classifier\"] = classifier # todo change back as soon as function\n",
        "\n",
        "  labels = get_labels(parameters, df_services, df_faqs, df_topics)\n",
        "  labels_embedded = model.encode(labels, convert_to_tensor=True)\n",
        "\n",
        "  if classifier == \"topics and services\":\n",
        "    # we need to get the translator\n",
        "    new_fibel = get_new_fibel(df_topics)\n",
        "\n",
        "  test_data = get_test_data(parameters, df_services, df_faqs, df_topics)\n",
        "  \n",
        "  # only get relevant test data\n",
        "  test_data = test_data[[\"questions\", \"true_answers\"]].iloc[:sample_size]\n",
        "\n",
        "  embedded_qs = []\n",
        "\n",
        "  # embed the questions\n",
        "  for q in test_data[\"questions\"].to_list():\n",
        "    # remove the stopwords\n",
        "    if parameters[\"remove_stopwords\"]:\n",
        "      q = stop_word_removal(q)\n",
        "    embedded_q = model.encode(q, convert_to_tensor=True)\n",
        "    embedded_qs.append(embedded_q)\n",
        "\n",
        "  idx = 0 \n",
        "  for row in test_data.iterrows():\n",
        "    question = row[1][\"questions\"]\n",
        "    true_answer = row[1][\"true_answers\"]\n",
        "    embedded_question = embedded_qs[idx]\n",
        "    cosine_scores = util.pytorch_cos_sim(embedded_question, labels_embedded)\n",
        "    \n",
        "    scores = cosine_scores[0].tolist()\n",
        "\n",
        "    mvps = pd.DataFrame(columns = [\"label\", \"score\"])\n",
        "    mvps[\"label\"] = labels\n",
        "    mvps[\"score\"] = scores\n",
        "\n",
        "    mvps = mvps.sort_values(by=\"score\",ascending=False)\n",
        "\n",
        "    mvps = mvps.iloc[:parameters[\"k\"]]\n",
        "\n",
        "    # the topics and services classifier goes even deeper\n",
        "\n",
        "    if parameters[\"classifier\"] == \"topics and services\":\n",
        "      new_mvps = pd.DataFrame(columns=[\"label\",\"score\"])\n",
        "      for label in mvps.iloc[:parameters[\"k_topics\"]][\"label\"].to_list():\n",
        "        possible_labels = list(new_fibel[label].values())\n",
        "        possible_labels_embedded = model.encode(possible_labels, convert_to_tensor=True)\n",
        "        cosine_scores = util.pytorch_cos_sim(embedded_question, possible_labels_embedded)\n",
        "        scores = cosine_scores[0].tolist()\n",
        "        mvps_here = pd.DataFrame(columns=[\"label\",\"score\"])\n",
        "        mvps_here[\"label\"] = possible_labels\n",
        "        mvps_here[\"score\"] = scores\n",
        "        new_mvps = new_mvps.append(mvps_here, ignore_index=True)\n",
        "\n",
        "      new_mvps = new_mvps.sort_values(by=\"score\",ascending=False)\n",
        "      new_mvps = new_mvps.drop_duplicates()\n",
        "      mvps = new_mvps.iloc[:parameters[\"k\"]]\n",
        "      \n",
        "\n",
        "    if true_answer in mvps[\"label\"].to_list():\n",
        "      hits += 1\n",
        "\n",
        "    idx += 1\n",
        "  \n",
        "  score = (hits / sample_size) * 100\n",
        "\n",
        "  classifier = parameters[\"classifier\"]\n",
        "  k = parameters[\"k\"]\n",
        "  k_topics = parameters[\"k_topics\"]\n",
        "\n",
        "  print(f\"The '{classifier}' Classifier has a Score of {int(score)}% if k equals {k} and k_topics equals {k_topics} with a sample size of {sample_size} \\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdkyYCFFAsah",
        "outputId": "c5947b6e-c5a3-4ef9-c7f8-0038b6f150ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'topics and services' Classifier has a Score of 71% if k equals 25 and k_topics equals 10 with a sample size of 1000 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invest_len_df = pd.DataFrame(columns=[\"query\", \"length\", \"correct\"])\n",
        "\n",
        "hits = 0\n",
        "\n",
        "labels = get_labels(parameters, df_services, df_faqs, df_topics)\n",
        "print(f\"The labels are of size {len(labels)} and are as follows:\")\n",
        "print(labels)\n",
        "test_data = get_test_data(parameters,df_services,df_faqs,df_topics)\n",
        "print(\"The test data is of shape\")\n",
        "print(test_data.shape)\n",
        "labels_embedded = embed_labels(labels,parameters,df_services,df_topics)\n",
        "\n",
        "# also, embed the topics separately\n",
        "if parameters[\"classifier\"] == \"topics and services\":\n",
        "  # we fist need to get the topics from the dictionary keys\n",
        "  topics_list = list(labels_embedded.keys())\n",
        "  # then embedd them\n",
        "  embedded_topics = model.encode(topics_list, convert_to_tensor=True)\n",
        "  # and then we also need the dictionary to look up the services for each topic\n",
        "  fibel = get_fibel(df_topics)\n",
        "\n",
        "for index,row in test_data.iloc[:parameters[\"how_many\"]].iterrows():\n",
        "  # get the user query\n",
        "  question = row[\"questions\"]\n",
        "  # remove the stopwords\n",
        "  if parameters[\"remove_stopwords\"]:\n",
        "    question = stop_word_removal(question)\n",
        "  # embedd the question\n",
        "  query_embedded = model.encode(question, convert_to_tensor=True)\n",
        "  # get the true answer\n",
        "  true_answer = row[\"true_answers\"]\n",
        "\n",
        "  if parameters[\"classifier\"] != \"topics and services\":\n",
        "    cosine_scores = util.pytorch_cos_sim(query_embedded, labels_embedded)\n",
        "    scores = cosine_scores[0].tolist()\n",
        "    score_df = pd.DataFrame(columns=[\"service\", \"score\"])\n",
        "    score_df[\"service\"] = labels\n",
        "    score_df[\"score\"] = scores\n",
        "    ranking = score_df.sort_values(by=\"score\",ascending=False)\n",
        "    mvps = ranking.iloc[:parameters[\"k\"]]\n",
        "  else:\n",
        "    cosine_scores = util.pytorch_cos_sim(query_embedded, embedded_topics)\n",
        "    scores = cosine_scores[0].tolist()\n",
        "    score_df = pd.DataFrame(columns=[\"service\", \"score\"])\n",
        "    score_df[\"service\"] = topics_list\n",
        "    score_df[\"score\"] = scores\n",
        "    ranking = score_df.sort_values(by=\"score\",ascending=False)\n",
        "    mvps = ranking.iloc[:parameters[\"k_topics\"]]\n",
        "\n",
        "    # this is if we not only classify topics, but also their services\n",
        "  \n",
        "    servs = pd.DataFrame(columns=[\"service\",\"score\"])\n",
        "    for i in mvps[\"service\"].to_list():\n",
        "      cosine_scores = util.pytorch_cos_sim(query_embedded, labels_embedded[i])\n",
        "      scores = cosine_scores[0].tolist()\n",
        "      score_df = pd.DataFrame(columns=[\"service\", \"score\"])\n",
        "      score_df[\"service\"] = public_get_service_names_from_topic_name(i, df_services,df_topics,fibel)\n",
        "      score_df[\"score\"] = scores\n",
        "      ranking = score_df.sort_values(by=\"score\",ascending=False)\n",
        "      mvps_for_each_topic = ranking.iloc[:parameters[\"k\"]]\n",
        "      servs = servs.append(mvps_for_each_topic)\n",
        "    servs = servs.sort_values(by=\"score\",ascending=False)\n",
        "\n",
        "    mvps = servs.iloc[:parameters[\"k\"]]\n",
        "\n",
        "  print(question)\n",
        "  print(true_answer)\n",
        "  print(mvps)\n",
        "\n",
        "  invest_len_dict = {}\n",
        "  invest_len_dict[\"query\"] = question\n",
        "  invest_len_dict[\"length\"] = len(question)\n",
        "\n",
        "  if true_answer in mvps['service'].to_list():\n",
        "    # increase the hits\n",
        "    # print(\"correctly classified\")\n",
        "    hits += 1\n",
        "    invest_len_dict[\"correct\"] = 1\n",
        "  else:\n",
        "    invest_len_dict[\"correct\"] = 0\n",
        "  # print(\"\\n\\n\")\n",
        "\n",
        "invest_len_df = invest_len_df.append(pd.Series(invest_len_dict),ignore_index=True)\n",
        "score = (hits/parameters[\"how_many\"])*100\n",
        "\n",
        "print(score)"
      ],
      "metadata": {
        "id": "teBRfk0vPcxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add the score\n",
        "parameters[\"score\"] = score\n",
        "# cast to series\n",
        "results_row = pd.Series(parameters)\n",
        "\n",
        "# save to frame\n",
        "results_path = \"/content/drive/MyDrive/Masterarbeit/Colab_Data/results_SCC.csv\"\n",
        "frame = pd.read_csv(results_path,index_col=False)\n",
        "frame = frame.append(results_row,ignore_index=True)\n",
        "frame.to_csv(results_path, index=False)"
      ],
      "metadata": {
        "id": "cqmT63ZQXWli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the service distribution for Section 5.4.4\n",
        "investigate = True\n",
        "\n",
        "if investigate and parameters[\"classifier\"] == \"services\":\n",
        "  threshold = 10\n",
        "  distr = test_data[\"d115Name\"].value_counts()\n",
        "  n = sum(distr.to_list())\n",
        "  distr_dict = dict(distr)\n",
        "  counter_irrelevant = 0\n",
        "  for key,value in distr_dict.items():\n",
        "    if value < threshold:\n",
        "      counter_irrelevant += 1\n",
        "    print(f\"{key}:{round((value/n)*100,2)}\")\n",
        "  print(\"\\n\\n\")\n",
        "  print(f\"{counter_irrelevant} services out of {len(distr.index.to_list())} selected services have been selected less than {threshold} times\\n\\n\")\n",
        "  distr.plot(kind=\"pie\")"
      ],
      "metadata": {
        "id": "VT9UkXz5gtr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# investigate the correlation between the sequence length and correct classifications\n",
        "\n",
        "invest_len = True\n",
        "\n",
        "if invest_len:\n",
        "\n",
        "  print(invest_len_df)\n",
        "  correlation = invest_len_df[\"correct\"].astype(float).corr(invest_len_df[\"length\"].astype(float))\n",
        "\n",
        "  print(f\"The correlation between the correctness of the SCC and the length of the user query is {round(correlation,4)}\")"
      ],
      "metadata": {
        "id": "UiP0eAe6lGzx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "YTLykc_7k8iY",
        "_BiSjpjxnw9M",
        "erRSPqU7nsJJ",
        "_sJnXjamn0iz",
        "OvM7eQcon4M2",
        "a-Z-N6__uRAy"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNyTCovx+Bi1NaTk1v7KkN+",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}